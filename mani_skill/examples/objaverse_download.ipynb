{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn7g17hc3ADi"
   },
   "source": [
    "# Objaverse Downloading Script\n",
    "\n",
    "Objaverse-XL is a Universe of 10M+ 3D Objects.\n",
    "It is hosted on ðŸ¤—[Hugging Face](https://huggingface.co/datasets/allenai/objaverse-xl) and includes a [Python API on GitHub](https://github.com/allenai/objaverse-xl).\n",
    "\n",
    "It's a bit difficult to get a dataset which is compatible for manipulation. There are a few subsets of Objaverse with additional annotations, such a Spock.\n",
    "\n",
    "See Spock [Readme](https://github.com/allenai/spoc-robot-training) and [notebook example](https://github.com/allenai/spoc-robot-training/blob/main/how_to_use_data.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: compress_json in /home/argusm/local/miniconda3/envs/paligemma/lib/python3.12/site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "# !export OBJAVERSE_PATH=\"/data/lmbraid19/argusm/datasets/spok/\"\n",
    "# !python -m objathor.dataset.download_annotations --version 2023_07_28 --path $OBJAVERSE_PATH\n",
    "# !python -m objathor.dataset.download_assets --version 2023_07_28 --path $OBJAVERSE_PATH\n",
    "!pip install objaverse --upgrade --quiet\n",
    "!pip install compress_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "wLB-BPGqGi2e",
    "outputId": "847c0b41-a4f7-413b-9af2-c0746ff64205"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.7'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import objaverse\n",
    "objaverse.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spock Annotations\n",
    "The rest of the cells are from the old tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "import pprint \n",
    "import numpy as np\n",
    "import compress_json\n",
    "import objaverse\n",
    "from IPython.display import Image, display\n",
    "\n",
    "OBJAVERSE_DIR = Path(\"/home/argusm/.objaverse/hf-objaverse-v1/\")\n",
    "\n",
    "#SPOCK_ANNOTATIONS_PATH = \"/data/lmbraid19/argusm/datasets/spok/2023_07_28/annotations.json.gz\"\n",
    "#annotations_spock = compress_json.load(SPOCK_ANNOTATIONS_PATH)\n",
    "\n",
    "SPOCK_ANNOTATIONS_PATH = \"/data/lmbraid19/argusm/datasets/spok/r2_dev/annotations.json\"\n",
    "SPOCK_MODLES = \"/data/lmbraid19/argusm/datasets/spok/r2_dev/assets\"\n",
    "\n",
    "with open(SPOCK_ANNOTATIONS_PATH, \"rb\") as f_obj:\n",
    "    annotations_spock = json.load(f_obj)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated examples 625252\n"
     ]
    }
   ],
   "source": [
    "print(\"annotated examples\", len(annotations_spock))\n",
    "annotations_spock_full = annotations_spock\n",
    "annotations_spock = dict(islice(annotations_spock.items(), 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Objaverse Objects\n",
    "Objaverse has a ton of assets, so let's filter them a bit for sanity:\n",
    "1. using existing annotation: `CanPickup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_pickups = []\n",
    "\n",
    "for i, (k,v) in enumerate(iter(annotations_spock.items())):\n",
    "    try:\n",
    "        can_pickup = v[\"thor_metadata\"][\"assetMetadata\"][\"primaryProperty\"] == \"CanPickup\"\n",
    "    except:\n",
    "        can_pickup = False\n",
    "    can_pickups.append(can_pickup)\n",
    "    \n",
    "print(\"can pickup:\", np.mean(can_pickups)*100,\"[%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metadata = False\n",
    "if print_metadata:\n",
    "    # print the metadata\n",
    "    #uid = '9d17119cc0f041e39b2a11211a677366'\n",
    "    uid = 'ff6c46b1e8f847ecadd5c95805f415c6'\n",
    "    pp.pprint(annotations_spock_full[uid])\n",
    "    ann = objaverse.load_annotations([uid])\n",
    "    pp.pprint(ann[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "\n",
    "glb_files = list((OBJAVERSE_DIR / \"glbs\").rglob(\"*.glb\"))\n",
    "print(f\"Objaverse models found {len(glb_files)} models in {OBJAVERSE_DIR}\")\n",
    "objaverse_files = dict([(x.stem, x) for x in glb_files])\n",
    "\n",
    "def get_glb_filename(uid):\n",
    "    return objaverse_files[uid]\n",
    "\n",
    "def get_glb_trimesh(uid):\n",
    "    scene = trimesh.load(get_glb_filename(uid))\n",
    "    return scene\n",
    "    \n",
    "def get_extents_file_m(uid):\n",
    "    scene = trimesh.load(get_glb_filename(uid))\n",
    "    geometries = list(scene.geometry.values())\n",
    "    if len(geometries) > 1:\n",
    "        print(f\"Warning: {len(geometries)} geometries in file for {uid}\")\n",
    "    geometry = geometries[0]\n",
    "    #print(geometry.extents)\n",
    "    #print(geometry.bounding_box.extents)\n",
    "    #print(geometry.bounding_box_oriented.extents)\n",
    "\n",
    "    extents = geometry.extents\n",
    "    return extents\n",
    "\n",
    "#uid = \"412ed49af0644f30bae822d29afbb066\"\n",
    "#print(get_extents_file_m(uid))\n",
    "#mesh = get_glb_trimesh(uid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Example Images with Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_uids = [\n",
    "'412ed49af0644f30bae822d29afbb066',\n",
    "'93128128f8f848d8bd261f6c1f763a53',\n",
    "'b5c9d06f19be4c92a1708515f6655573',\n",
    "'584ce7acb3384c36bf252fde72063a56',\n",
    "'2796ec7a4a324d9e988d95d88bb6f1e2',\n",
    "'005a246f8c304e77b27cf11cd53ff4ed',\n",
    "'013176cfbc8145a0b10c3191ac265e8b',\n",
    "'00bfa4e5862d4d4b89f9bcf06d2a19e4',\n",
    "'088c1883e07e4946956488171e3a06bf',\n",
    "'0364ab96f338493c972248102b462aa4',\n",
    "'ff6c46b1e8f847ecadd5c95805f415c6']\n",
    "\n",
    "OBJAVERSE_SCALES_GUESS = {\n",
    "    'b5c9d06f19be4c92a1708515f6655573':.02,\n",
    "    '412ed49af0644f30bae822d29afbb066':.001,\n",
    "    '088c1883e07e4946956488171e3a06bf':.1,\n",
    "    '93128128f8f848d8bd261f6c1f763a53':.005\n",
    "}\n",
    "\n",
    "\n",
    "def find_index_with_largest_size_above_min(thubnail_image_data, min_size=448):\n",
    "    valid_entries = [\n",
    "        (index, entry['size']) \n",
    "        for index, entry in enumerate(thubnail_image_data) \n",
    "        if max(entry['width'], entry['height']) >= min_size\n",
    "    ]\n",
    "    if not valid_entries:\n",
    "        return 0 # No valid entries found\n",
    "    return min(valid_entries, key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "def pprint_extents(extents):\n",
    "    extents = np.array(extents)*100\n",
    "    return \" \".join([f\"{d:.1f}\" for d in extents]) + \" cm\"\n",
    "\n",
    "def print_spock_metadata(uid, metadata, extents_thor):\n",
    "    print(\"category\", metadata[\"category\"])\n",
    "    print(\"descrip\", metadata[\"description\"])\n",
    "    #print(\"descr-auto\", metadata[\"description_auto\"])\n",
    "    print(\"scale_thor\", metadata[\"scale\"], \"extents\", pprint_extents(extents_thor), \"\\t(apply scale_thor to extents_thor)\")\n",
    "\n",
    "def get_extents_meta_m(anno_spock):\n",
    "    bbox = anno_spock['thor_metadata']['assetMetadata']['boundingBox']\n",
    "    extents = {\n",
    "        'x': bbox['max']['x'] - bbox['min']['x'],\n",
    "        'y': bbox['max']['y'] - bbox['min']['y'],\n",
    "        'z': bbox['max']['z'] - bbox['min']['z']\n",
    "    }\n",
    "    extents = [extents[d] for d in list('xyz')]\n",
    "    return extents\n",
    "\n",
    "\n",
    "filter_pass_count = 0\n",
    "show_image = True\n",
    "for i, uid in enumerate(annotations_spock.keys()):\n",
    "#for i, uid in enumerate(good_uids):\n",
    "    try:\n",
    "        anno_s = annotations_spock_full[uid]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        extents_thor = get_extents_meta_m(anno_s)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    if min(extents_thor) < 2/100 or max(extents_thor) > 50/100:\n",
    "        continue  # filter object too large to grasp\n",
    "\n",
    "    if show_image:\n",
    "        print_spock_metadata(uid, anno_s, extents_thor)\n",
    "        ann = objaverse.load_annotations([uid])\n",
    "        i = find_index_with_largest_size_above_min(ann[uid]['thumbnails']['images'])\n",
    "        url = ann[uid]['thumbnails']['images'][i]['url']\n",
    "        display(Image(url=url, width=512))\n",
    "    print()\n",
    "\n",
    "    filter_pass_count += 1\n",
    "    #results = objaverse.load_objects([key])\n",
    "    if i > 100:\n",
    "        break\n",
    "\n",
    "print(\"pass rate\", filter_pass_count, \"/\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ManiSkill Scale\n",
    "\n",
    "Find which scales we have to give ManiSkill for objaverse objects so that they are loaded ok.\n",
    "The default THOR metadata bounding box needs to be combined with the trimesh bounding box in order to find the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_uids = [\n",
    "'412ed49af0644f30bae822d29afbb066',\n",
    "'93128128f8f848d8bd261f6c1f763a53',\n",
    "'b5c9d06f19be4c92a1708515f6655573',\n",
    "'584ce7acb3384c36bf252fde72063a56',\n",
    "'2796ec7a4a324d9e988d95d88bb6f1e2',\n",
    "'005a246f8c304e77b27cf11cd53ff4ed',\n",
    "'013176cfbc8145a0b10c3191ac265e8b',\n",
    "'00bfa4e5862d4d4b89f9bcf06d2a19e4',\n",
    "'088c1883e07e4946956488171e3a06bf',\n",
    "'0364ab96f338493c972248102b462aa4',\n",
    "'ff6c46b1e8f847ecadd5c95805f415c6']\n",
    "\n",
    "OBJAVERSE_SCALES_GUESS = {\n",
    "    'b5c9d06f19be4c92a1708515f6655573':.02,\n",
    "    '412ed49af0644f30bae822d29afbb066':.001,\n",
    "    '088c1883e07e4946956488171e3a06bf':.1,\n",
    "    '93128128f8f848d8bd261f6c1f763a53':.005\n",
    "}\n",
    "\n",
    "\n",
    "def pprint_extents(extents):\n",
    "    extents = np.array(extents)*100\n",
    "    return \" \".join([f\"{d:.1f}\" for d in extents]) + \" cm\"\n",
    "\n",
    "def print_spock_metadata(uid, metadata, extents_thor):\n",
    "    print(\"category\", metadata[\"category\"])\n",
    "    #print(\"descrip\", metadata[\"description\"])\n",
    "    #print(\"descr-auto\", metadata[\"description_auto\"])\n",
    "    print(\"scale_thor\", metadata[\"scale\"], \"extents\", pprint_extents(extents_thor), \"\\t(apply scale_thor to extents_thor)\")\n",
    "\n",
    "def get_extents_meta_m(anno_spock):\n",
    "    bbox = anno_spock['thor_metadata']['assetMetadata']['boundingBox']\n",
    "    extents = {\n",
    "        'x': bbox['max']['x'] - bbox['min']['x'],\n",
    "        'y': bbox['max']['y'] - bbox['min']['y'],\n",
    "        'z': bbox['max']['z'] - bbox['min']['z']\n",
    "    }\n",
    "    extents = [extents[d] for d in list('xyz')]\n",
    "    return extents\n",
    "\n",
    "def guess_scale(extents_thor, extents_file):\n",
    "    min_t = min(extents_thor)\n",
    "    min_f = min(extents_file)\n",
    "    return min_t/min_f\n",
    "\n",
    "filter_pass_count = 0\n",
    "show_image = False\n",
    "#for i, key in enumerate(annotations_spock.keys()):\n",
    "for i, uid in enumerate(good_uids):\n",
    "    try:\n",
    "        anno_s = annotations_spock_full[uid]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        extents_thor = get_extents_meta_m(anno_s)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    if min(extents_thor) < 2/100 or max(extents_thor) > 50/100:\n",
    "        continue  # filter object too large to grasp\n",
    "\n",
    "    print(\"uid\", uid)\n",
    "    extents_file = get_extents_file_m(uid)\n",
    "    \n",
    "    print_spock_metadata(uid, anno_s, extents_thor)\n",
    "    scale_thor = anno_s[\"scale\"]\n",
    "    scale_pred = guess_scale(extents_thor, extents_file)\n",
    "    print(\"                extents\", pprint_extents(extents_file*scale_thor), \"\\t(apply scale_thor to extents_file)\")\n",
    "    print(\"                extents\", pprint_extents(extents_file*scale_pred), \"\\t(apply scale_pred to extents_file)\")\n",
    "    print(f\"scale_pred {scale_pred:.5f}\")\n",
    "    if uid in OBJAVERSE_SCALES_GUESS:\n",
    "        print(f\"scale_gues {OBJAVERSE_SCALES_GUESS[uid]:.5f}\")\n",
    "\n",
    "    if show_image:\n",
    "        ann = objaverse.load_annotations([uid])\n",
    "        url = ann[uid]['thumbnails']['images'][0]['url']\n",
    "        display(Image(url=url, width=512))\n",
    "    print()\n",
    "\n",
    "    filter_pass_count += 1\n",
    "    #results = objaverse.load_objects([key])\n",
    "    if i > 100:\n",
    "        break\n",
    "\n",
    "print(\"pass rate\", filter_pass_count, \"/\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Objects by Droid Text Annotations\n",
    "\n",
    "Make use of droid text annotations to find which objects are most frequent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"droid_objects.txt\") as f_obs\n",
    "  text = f_obs.read()\n",
    "  \n",
    "items = []\n",
    "for i, x in enumerate(text.split(\"\\n\")):\n",
    "    item = x.split(\" \")[3]\n",
    "    item = item.replace(\"\\t\",\"\")\n",
    "    items.append(item)\n",
    "    if item.endswith(\"s\"):\n",
    "        items.append(item[:-1])\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize counters for categories and items\n",
    "category_counter = Counter()\n",
    "item_counter = Counter(items)\n",
    "\n",
    "# Iterate through annotations and count categories and items\n",
    "for uid, anno_s in annotations_spock_full.items():\n",
    "    category = anno_s[\"category\"]\n",
    "    category_counter[category] += 1\n",
    "\n",
    "\n",
    "# Print the unique categories that are also in the items list\n",
    "categories_set = set(category_counter.keys())\n",
    "\n",
    "item_intersection = categories_set.intersection(set(items))\n",
    "for x in item_intersection:\n",
    "    print(x.ljust(14), str(category_counter[x]).ljust(10), item_counter[x])\n",
    "\n",
    "# Print frequency counts\n",
    "#print(\"Category Frequency:\", category_counter)\n",
    "#print(\"Item Frequency:\", item_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dowload Object: Objaverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import objaverse\n",
    "collection = good_uids  # list of UIDs\n",
    "results = objaverse.load_objects(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Objects: R2-dev\n",
    "\n",
    "Look at the pre-release r2-dev objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "source = \"r2_dev\"\n",
    "if source == \"objaverse\":\n",
    "    res = objaverse.load_objects(good_uids)\n",
    "    print(res)\n",
    "elif source == \"r2_dev\":\n",
    "    for uid in tqdm(good_uids):\n",
    "        url = f\"https://***REMOVED***.r2.dev/assets/{uid}.tar\"\n",
    "        fn = f\"{SPOCK_MODLES}/{uid}.tar\"\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)  # Use streaming for large files\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except requests.exceptions.HTTPError:\n",
    "            continue\n",
    "        \n",
    "        with open(fn, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):  # Write in chunks\n",
    "                file.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MSGPack Files\n",
    "\n",
    "Try to figure out what is happening here, see if we want to load the msgpack or the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "print(SPOCK_MODLES)\n",
    "\n",
    "uid = \"ff6c46b1e8f847ecadd5c95805f415c6\"\n",
    "#uid = \"412ed49af0644f30bae822d29afbb066\"\n",
    "\n",
    "msg_path = f\"{SPOCK_MODLES}/{uid}/{uid}.msgpack\"\n",
    "with open(msg_path, \"rb\") as data_file:\n",
    "    byte_data = data_file.read()\n",
    "data_loaded = msgpack.unpackb(byte_data)\n",
    "for key in data_loaded:\n",
    "    sample = \"\"\n",
    "    if type(data_loaded[key]) == list:\n",
    "        sample = (len(data_loaded[key]),data_loaded[key][0])\n",
    "    if isinstance(data_loaded[key], (bool, str,dict)):\n",
    "        sample = data_loaded[key]\n",
    "\n",
    "    print(key, type(data_loaded[key]), sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(get_glb_filename(uid))\n",
    "mesh = get_glb_trimesh(uid)\n",
    "tmp = list(mesh.geometry.values())[0]\n",
    "print(\"vertices\", tmp.vertices.shape)\n",
    "print(\"triangls\", tmp.triangles.shape)\n",
    "print(\"normals\", tmp.face_normals.shape)\n",
    "print(\"material\", tmp.visual.material)\n",
    "print(\"uv\", tmp.visual.uv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygltflib\n",
    "from pygltflib import GLTF2\n",
    "glb_filename = get_glb_filename(uid)\n",
    "glb = GLTF2().load(glb_filename)  # load method auto detects based on extension\n",
    "#print(glb.asset)\n",
    "#glb.bufferViews\n",
    "#glb.images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Images (VLM Questioning)\n",
    "\n",
    "Download the pre-rendered images of the objects from the hosting website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "OBJAVERSE_THUMBNAIL_PATH = Path('/data/lmbraid19/argusm/datasets/spok/thumbnails')\n",
    "\n",
    "def find_index_with_largest_size_above_min(thubnail_image_data, min_size=448):\n",
    "    valid_entries = [\n",
    "        (index, entry['size']) \n",
    "        for index, entry in enumerate(thubnail_image_data) \n",
    "        if max(entry['width'], entry['height']) >= min_size\n",
    "    ]\n",
    "    if not valid_entries:\n",
    "        return 0 # No valid entries found\n",
    "    return min(valid_entries, key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "for i, (uid, value) in tqdm(enumerate(annotations_spock.items()),total=len(annotations_spock)):\n",
    "    obj_anno = annotations_spock[uid]\n",
    "    if \"thor_metadata\" not in obj_anno:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        anno_s = annotations_spock_full[uid]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        extents_thor = get_extents_meta_m(anno_s)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    if min(extents_thor) < 2/100 or max(extents_thor) > 50/100:\n",
    "        continue  # filter object too large to grasp\n",
    "        \n",
    "    ann = objaverse.load_annotations([uid])\n",
    "    thumbnail_image_data = ann[uid]['thumbnails']['images']\n",
    "    i = find_index_with_largest_size_above_min(thumbnail_image_data)\n",
    "    url = thumbnail_image_data[i]['url']\n",
    "    fn = OBJAVERSE_THUMBNAIL_PATH / f\"{uid}{Path(url).suffix}\"\n",
    "    if fn.is_file():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)  # Use streaming for large files\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    except requests.exceptions.HTTPError:\n",
    "        continue\n",
    "    \n",
    "    with open(fn, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):  # Write in chunks\n",
    "            file.write(chunk)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Old-Stuff] Render in Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gymnasium as gym\n",
    "import mani_skill.examples.clevr_env  # do import to register env, not used otherwise\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "from mani_skill.examples.gen_dataset import Args, reset_random\n",
    "import matplotlib.pyplot as plt \n",
    "    \n",
    "args = Args()\n",
    "args.env_id = \"ClevrMove-v1\"\n",
    "#args.shader = \"rt-fast\" or \"rt\"\n",
    "#args.seed = [42,]\n",
    "reset_random(args)\n",
    "\n",
    "vis = False\n",
    "env: BaseEnv = gym.make(\n",
    "    args.env_id,\n",
    "    obs_mode=args.obs_mode,\n",
    "    reward_mode=args.reward_mode,\n",
    "    control_mode=args.control_mode,\n",
    "    render_mode=args.render_mode,\n",
    "    sensor_configs=dict(shader_pack=args.shader),\n",
    "    human_render_camera_configs=dict(shader_pack=args.shader),\n",
    "    viewer_camera_configs=dict(shader_pack=args.shader),\n",
    "    num_envs=args.num_envs,\n",
    "    sim_backend=args.sim_backend,\n",
    "    parallel_in_single_scene=False,\n",
    "    robot_uids=\"panda_wristcam\",\n",
    "    object_dataset=\"objaverse\",\n",
    "    scene_dataset=\"Table\",\n",
    "    # **args.env_kwargs\n",
    ")\n",
    "\n",
    "for i in range(10**6):\n",
    "    obs, _ = env.reset(seed=args.seed[0], options=dict(reconfigure=True))\n",
    "    if args.seed is not None:\n",
    "        env.action_space.seed(args.seed[0])\n",
    "    if vis and args.render_mode is not None:\n",
    "        viewer = env.render()\n",
    "        if isinstance(viewer, sapien.utils.Viewer):\n",
    "            viewer.paused = args.pause\n",
    "        env.render()\n",
    "    else:\n",
    "        env.render()\n",
    "    \n",
    "    # get before image\n",
    "    images = env.base_env.scene.get_human_render_camera_images('render_camera')\n",
    "    print(i)\n",
    "    plt.imshow(images['render_camera'][0].detach().cpu().numpy())\n",
    "    plt.show()\n",
    "    if i > 0:\n",
    "        break\n",
    "    reset_random(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Old-Stuff] Load all objaverse UIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = objaverse.load_uids()\n",
    "len(uids), type(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a couple of minutes\n",
    "annotations = objaverse.load_annotations(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pprint \n",
    "for i, (uid, annotation) in enumerate(annotations.items()):\n",
    "    pp.pprint(annotation)\n",
    "    #print(annotation[\"description\"])\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Old-Stuff] LIVS Annotation Stuff\n",
    "\n",
    "LVIS was this subset of objaverse which hand proper annotations. I was looking at it before I found the Spock annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k9kk352jAcE"
   },
   "outputs": [],
   "source": [
    "import objaverse\n",
    "counts = [(k,len(v)) for k,v in annotations2.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select N_categories objects\n",
    "N_categories = 100\n",
    "print(len(counts))\n",
    "sum([v for k, v in counts])\n",
    "collection = []\n",
    "i = 0\n",
    "for i, (k, v) in enumerate(annotations2.items()):\n",
    "  if i >= N_categories:\n",
    "    break\n",
    "  collection.append(v[0])\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = objaverse.load_objects(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object loading code, LVIS names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glb_downloaded = [x.stem for x in glb_files]\n",
    "# glb_downloaded_set = set(glb_downloaded)            \n",
    "# anno_path = objaverse_folder / \"lvis-annotations.json.gz\"\n",
    "# import gzip, json\n",
    "# with gzip.open(anno_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "#     lvis_annotations = json.load(f)\n",
    "# mapping = {}\n",
    "# for category, items in lvis_annotations.items():\n",
    "#     for index, item in enumerate(items, start=1):  # Use 1-based indexing\n",
    "#         if item not in glb_downloaded_set:\n",
    "#             continue\n",
    "#         mapping[f\"{category}-{index}\"] = glb_files[glb_downloaded.index(item)]\n",
    "# self.objaverse_model_ids = list(mapping.keys())\n",
    "# self.objaverse_files = mapping"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
